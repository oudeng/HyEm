Metadata-Version: 2.4
Name: hyem-repro
Version: 0.1.0
Summary: Reproducibility package for HYEM (Indexable Hyperbolic Retrieval for Biomedical Ontologies)
Author: Ou Deng and collaborators
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown

# HyEm: Radius-Controlled Hyperbolic Retrieval with Tangent-Space Indexing

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the **reproducibility package** for the paper:

> **HyEm: Radius-Controlled Hyperbolic Retrieval with Tangent-Space Indexing for Biomedical Ontologies**  
> Ou Deng, Shoji Nishimura, Atsushi Ogihara, Qun Jin  
> Graduate School of Human Sciences / Faculty of Human Sciences, Waseda University

---

## Overview

HyEm is a query-adaptive hyperbolic retrieval layer designed for biomedical ontology grounding in RAG (Retrieval-Augmented Generation) systems. It addresses two key challenges:

1. **Indexability**: Hyperbolic nearest-neighbor search is not a standard primitive in vector databases
2. **Query heterogeneity**: Not every query benefits from hyperbolic geometry

### Key Features

- **Radius-controlled hyperbolic embeddings** that prevent boundary pathologies
- **Tangent-space indexing** compatible with standard Euclidean ANN (e.g., HNSW/FAISS)
- **Query-adaptive soft mixing** between Euclidean and hyperbolic scores
- **Lightweight and reproducible** experimental pipeline

### Method Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HyEm Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Offline:                                                           â”‚
â”‚    Ontology nodes v â†’ Hyperbolic embedding x_v â†’ Tangent vector u_v â”‚
â”‚                        (radius-controlled)       (stored in ANN)    â”‚
â”‚                                                                     â”‚
â”‚  Online:                                                            â”‚
â”‚    Query q â†’ Text embedding e_q â†’ Adapter â†’ u_q                     â”‚
â”‚                    â†“                          â†“                     â”‚
â”‚              Gate Î±(q)              ANN search â†’ Candidates C       â”‚
â”‚                    â†“                          â†“                     â”‚
â”‚              Soft mixing: score = Î±Â·s_H + (1-Î±)Â·s_E                 â”‚
â”‚                                      â†“                              â”‚
â”‚                              Reranked top-k                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start

### 1. Environment Setup

**Tested with:** Python 3.9â€“3.11, PyTorch 2.0+

```bash
# Option A: conda (recommended)
conda create -n hyem python=3.10 -y
conda activate hyem
pip install -r requirements.txt
pip install -e .

# Option B: venv
python -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
pip install -e .
```

### 2. Download Data

```bash
python scripts/00_download_data.py --datasets hpo do
# Optional (may require manual download):
# python scripts/00_download_data.py --datasets mesh
```

### 3. Run Experiments

See the **Experiments** section below for detailed instructions.

---

## Repository Structure

```
HyEm/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml              # Package configuration
â”‚
â”œâ”€â”€ scripts/                    # Numbered pipeline scripts (00-11)
â”‚   â”œâ”€â”€ 00_download_data.py     # Download HPO/DO/MeSH ontologies
â”‚   â”œâ”€â”€ 01_build_subset.py      # Build size-controlled subsets
â”‚   â”œâ”€â”€ 02_build_queries.py     # Generate Q-E/Q-H/Q-M benchmarks
â”‚   â”œâ”€â”€ 03_encode_text.py       # Encode texts with SentenceTransformers
â”‚   â”œâ”€â”€ 04_train_embeddings.py  # Train hyperbolic & Euclidean embeddings
â”‚   â”œâ”€â”€ 05_train_adapters.py    # Train query adapters
â”‚   â”œâ”€â”€ 06_train_gate.py        # Train query-adaptive gate
â”‚   â”œâ”€â”€ 07_build_indexes.py     # Build HNSW indexes
â”‚   â”œâ”€â”€ 08_eval_retrieval.py    # Evaluate retrieval (main results)
â”‚   â”œâ”€â”€ 09_indexability_test.py # Tangent-space recall stress test
â”‚   â”œâ”€â”€ 10_efficiency_benchmark.py # Latency & memory benchmarks
â”‚   â””â”€â”€ 11_make_paper_artifacts.py # Export LaTeX tables & figures
â”‚
â”œâ”€â”€ exp_basic/                  # Basic experiments (main paper)
â”‚   â”œâ”€â”€ README_basic.md         # Instructions for basic experiments
â”‚   â”œâ”€â”€ run_basic_pipeline.sh   # Full pipeline (single dataset)
â”‚   â”œâ”€â”€ run_basic_analysis.sh   # Indexability & efficiency analysis
â”‚   â””â”€â”€ run_paper_artifacts.sh  # Generate LaTeX tables & figures
â”‚
â”œâ”€â”€ exp_ext1/                   # Extended experiments 1
â”‚   â”œâ”€â”€ README_ext1.md          # Instructions for extended experiments
â”‚   â”œâ”€â”€ run_qe_biomed_encoder.sh    # Biomedical encoder experiment
â”‚   â”œâ”€â”€ run_scale_20k.sh        # Scale-up to 20k nodes
â”‚   â”œâ”€â”€ run_radius_sweep.sh     # Radius-budget sweep
â”‚   â””â”€â”€ plot_indexability_with_theory.py # Theory-guided plotting
â”‚
â”œâ”€â”€ exp_ext2/                   # Extended experiments 2 (reviewer-motivated)
â”‚   â”œâ”€â”€ README_ext2.md          # Instructions for extended experiments
â”‚   â”œâ”€â”€ run_qe_high_baseline.sh # Biomedical encoder + synonym indexing for Q-E
â”‚   â”œâ”€â”€ run_adapter_ablation.sh # Linear vs 2-layer MLP adapter
â”‚   â””â”€â”€ run_hyper_encoder_compare.sh # Tangent-space HGCN baseline
â”‚
â”œâ”€â”€ src/hyem/                   # Core library
â”‚   â”œâ”€â”€ ontology/               # OBO parsing & graph utilities
â”‚   â”œâ”€â”€ text/                   # Text embedding (SentenceTransformers)
â”‚   â”œâ”€â”€ models/                 # Hyperbolic embeddings, adapters, gates
â”‚   â”œâ”€â”€ indexing/               # HNSW wrapper (hnswlib)
â”‚   â”œâ”€â”€ retrieval/              # Retrieval methods & soft mixing
â”‚   â””â”€â”€ eval/                   # Metrics & evaluation tasks
â”‚
â”œâ”€â”€ data/                       # Data directory (created during experiments)
â”‚   â”œâ”€â”€ raw/<dataset>/          # Downloaded .obo files
â”‚   â””â”€â”€ processed/<dataset>/<subset>_seed<seed>/
â”‚       â”œâ”€â”€ nodes.jsonl, edges.csv  # Graph structure
â”‚       â”œâ”€â”€ queries_*.jsonl     # Benchmark queries
â”‚       â”œâ”€â”€ emb_*.npy           # Text embeddings
â”‚       â”œâ”€â”€ u_hyem.npy          # Hyperbolic graph embeddings
â”‚       â”œâ”€â”€ adapter_*.pt        # Trained adapters
â”‚       â”œâ”€â”€ indexes/            # HNSW index files
â”‚       â”œâ”€â”€ results/            # Evaluation results
â”‚       â””â”€â”€ analysis/           # Indexability & efficiency analysis
â”‚
â””â”€â”€ paper_artifacts/            # Generated LaTeX snippets & figures
```

---

## Experiments

### Basic Experiments (Main Paper)

The basic experiments reproduce the main results on HPO-5k and DO-5k.

**Quick Start:**
```bash
# Run complete pipeline for HPO
bash exp_basic/run_basic_pipeline.sh hpo 5000 0 cuda

# Run analysis benchmarks
bash exp_basic/run_basic_analysis.sh hpo 5000 0 cuda

# Generate paper artifacts
bash exp_basic/run_paper_artifacts.sh "hpo do" 5000 0
```

ğŸ“– **Full instructions:** [exp_basic/README_basic.md](exp_basic/README_basic.md)

---

### Extended Experiments 1

Extended experiments address reviewer concerns and provide additional evidence:

| Experiment | Purpose | Command |
|------------|---------|---------|
| **Biomedical Encoder** | Non-trivial Q-E baseline | `bash exp_ext1/run_qe_biomed_encoder.sh ...` |
| **Scale-up (20k)** | Evidence beyond 5k nodes | `bash exp_ext1/run_scale_20k.sh ...` |
| **Radius Sweep** | Validate theory bounds | `bash exp_ext1/run_radius_sweep.sh ...` |
| **Theory Plot** | Visualize Îº(R) bound | `python exp_ext1/plot_indexability_with_theory.py ...` |

ğŸ“– **Full instructions:** [exp_ext1/README_ext1.md](exp_ext1/README_ext1.md)

---

## Datasets

| Dataset | Description | Source |
|---------|-------------|--------|
| **HPO** | Human Phenotype Ontology | [hpo.jax.org](https://hpo.jax.org/) |
| **DO** | Disease Ontology | [disease-ontology.org](https://disease-ontology.org/) |
| **MeSH** | Medical Subject Headings (optional) | [nlm.nih.gov/mesh](https://www.nlm.nih.gov/mesh/) |

> **Note:** SNOMED CT and ICD are not included due to licensing constraints.

---

## Query Taxonomy

HyEm evaluates on three query families:

| Type | Description | Example | Primary Signal |
|------|-------------|---------|----------------|
| **Q-E** | Entity-centric | "What is dilated cardiomyopathy?" | Euclidean similarity |
| **Q-H** | Taxonomy-navigation | "What are subtypes of cardiomyopathy?" | Hyperbolic distance |
| **Q-M** | Mixed-intent | "Diseases similar to X at same specificity" | Soft mixing |

---

## Key Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--dim` | 32 | Embedding dimension |
| `--radius_budget` | 3.0 | Maximum radius in tangent space (controls indexability) |
| `--L_h` | 200 | Hyperbolic candidate oversampling factor |
| `--L_e` | 200 | Euclidean candidate oversampling factor |
| `--epochs` | 10 (graph), 5 (adapter/gate) | Training epochs |

**Guidance from theory:**
- Larger `radius_budget` â†’ better hierarchy representation, worse tangent-space approximation
- Distortion factor Îº(R) = sinh(R)/R bounds the approximation error

---

## Output Files

### Main Results
- `results/summary.csv` â€” Aggregated metrics (Hits@k, MRR, F1) per method and query type
- `results/per_query_*.csv` â€” Per-query results for significance tests

### Analysis
- `analysis/indexability_recall_curve.csv` â€” Recall@k vs. oversampling L
- `analysis/efficiency.csv` â€” Query latency and index size

### Paper Artifacts
- `paper_artifacts/<dataset>/rows_*.tex` â€” LaTeX table rows

---

## Troubleshooting

### Common Issues

1. **SentenceTransformers download fails**
   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```

2. **CUDA out of memory**
   ```bash
   # Use CPU instead
   bash exp_basic/run_basic_pipeline.sh hpo 5000 0 cpu
   ```

3. **MeSH download fails**
   - Download manually from [NLM](https://www.nlm.nih.gov/mesh/filelist.html)
   - Place at `data/raw/mesh/mesh.obo`

---

## Citation

If you use this code, please cite:

```bibtex
@article{deng2026hyem,
  title={HyEm: Radius-Controlled Hyperbolic Retrieval with Tangent-Space Indexing for Biomedical Ontologies},
  author={Deng, Ou and Nishimura, Shoji and Ogihara, Atsushi and Jin, Qun},
  journal={[arXiv]},
  year={2026}
}
```

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.
